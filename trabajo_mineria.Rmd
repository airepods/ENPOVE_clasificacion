---
title: "Trabajo final mineria"
output:
  html_document:
    df_print: paged
  pdf_document: default
  html_notebook: default
---

Limpiar el workspace, la consola y cambiar el directorio de trabajo
```{r}
rm(list = ls())
graphics.off()
cat("\014")
options(scipen=999)
options(digits = 3)
# cambiar directiorio
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
```
Se cargan los paquetes
```{r}
library(pacman)
p_load(foreign, DataExplorer, recipes, MLmetrics, caret, 
       caTools, yardstick, funModeling, DataExplorer, VIM)

```


Cargar el conjunto de datos
```{r}
datos <- read.csv("enpove_encuesta.csv")
```

Conversion de las variables al tipo de dato apropiado
```{r}
str(datos)

# No considerar la variable de identificaciÃ³n ID
datos$CODIGO_PERSONA <- NULL 

# Etiquetando las opciones de las variables categÃ³ricas
datos$Dia <- as.numeric(datos$Dia)
datos$Mes <- as.numeric(datos$Mes)
datos$Anio <- as.numeric(datos$Anio)

datos$P304 <- as.factor(datos$P304)
datos$P311 <- as.factor(datos$P311)
datos$P313 <- as.factor(datos$P313)
datos$TIENE_SEGURO <- as.factor(datos$TIENE_SEGURO)
datos$P402 <- as.factor(datos$P402)

datos$P408_1 <- as.factor(datos$P408_1)
datos$P408_2 <- as.factor(datos$P408_2)
datos$P408_3 <- as.factor(datos$P408_3)
datos$P408_6 <- as.factor(datos$P408_6)

datos$NOMBDEPA <- as.factor(datos$NOMBDEPA)
datos$NOMBPROV <- as.factor(datos$NOMBPROV)
datos$NOMBDIST <- as.factor(datos$NOMBDIST)

datos$P501 <- as.factor(datos$P501)
datos$P504 <- as.factor(datos$P504)
datos$P601 <- as.factor(datos$P601)
datos$P609 <- as.factor(datos$P609)
datos$P627 <- as.factor(datos$P627)

#target
###################################
datos$P701 <- as.factor(datos$P701)
###################################

datos$P703 <- as.factor(datos$P703)
datos$P706 <- as.factor(datos$P706)
datos$P807 <- as.factor(datos$P807)

datos$Sexo <- as.factor(datos$Sexo)
datos$Edad <- as.numeric(datos$Edad)
datos$Migro_de_venezuela <- as.factor(datos$Migro_de_venezuela)


str(datos)

```
Colocar los nombres a las categorias
```{r}
datos$Dia <- as.numeric(datos$Dia)
datos$Mes <- as.numeric(datos$Mes)
datos$Anio <- as.numeric(datos$Anio)

levels(datos$P304) <- c("Si", "No")
levels(datos$P311) <- c("Si", "No")
levels(datos$P313) <- c("Si", "No")
levels(datos$TIENE_SEGURO) <- c("1", "2", "3", "5")
levels(datos$P402) <- c("Si", "No")

levels(datos$P408_1) <- c("Si", "No")
levels(datos$P408_2) <- c("Si", "No")
levels(datos$P408_3) <- c("Si", "No")
levels(datos$P408_6) <- c("Si", "No")

levels(datos$P501) <- c("Sin nivel", "Preescolar", "Educacion Básica Incompleta", "Educacion Basica Completa", "Educacion Media Diversificada Incompleta", "Educacion Media Diversificada Completa", "Tecnico Superior Incompleta", "Tecnico Superior Completa", "Superior Universitaria Incompleta", "Superior Universitaria Completa", "Maestria/ Doctorado")
levels(datos$P504) <- c("Si", "No")
levels(datos$P601) <- c("Si", "No")
levels(datos$P609) <- c("Si", "No")
levels(datos$P627) <- c("Si", "No")

#target
###################################
levels(datos$P701) <- c("Si", "No")
###################################

levels(datos$P703) <- c("Si", "No")
levels(datos$P706) <- c("Si", "No")
levels(datos$P807) <- c("Si", "No")

levels(datos$Sexo) <- c("Hombre", "Mujer")
levels(datos$Migro_de_venezuela) <- c("Si", "No")

str(datos)
```
Analisis de variables
```{r}
tapply(datos$Dia, datos$P701, mean)

# 15.4 15.6

plotar(datos, 
       input = "Dia", 
       target="P701", 
       plot_type="histdens")

```
```{r}
tapply(datos$Mes, datos$P701, mean)

#   Si   No 
#  6.74 6.75

plotar(datos, 
       input = "Mes", 
       target="P701", 
       plot_type="histdens")
```

```{r}
tapply(datos$Anio, datos$P701, mean)

#  Si   No 
# 1989 1989

plotar(datos, 
       input = "Anio", 
       target="P701", 
       plot_type="histdens")
```
Las variables Dia, mes y anio no van en el modelo


```{r}
datos$NOMBDIST <- NULL

dependiente <- "P701"
solo_cat <- lapply(Filter(is.factor, datos), levels)
predictores <- setdiff(names(solo_cat), dependiente)
predictores

cross_plot(datos, 
           input     = predictores, 
           target    = dependiente,
           plot_type = "percentual") 
```

```{r}
tapply(datos$Edad, datos$P701, mean)

#  Si   No 
# 29.3 28.6 

plotar(datos, 
       input = "Edad", 
       target="P701", 
       plot_type="histdens")

```
Variables descartadas

* Anio
* Mes
* Dia
* P304
* TIENE_SEGURO
* P408_1
* P408_2
* NOMBPROV
* NOMBDIST
* P504
* P609
* P703
* Sexo
* Migro_venezuela


Variables seleccionadas

* Numericas 
  + **Edad** - Edad del entrevistado
* Categoricas
  + **P311** - DESDE QUE LLEGÓ AL PERÚ. ¿HA VIVIDO SIEMPRE EN ESTE DISTRITO?
  + **P313** - USTED, ¿PIENSA QUEDARSE A VIVIR EN PERÚ?
  + **P402** - ¿PADECE DE ALGUNA ENFERMEDAD O MALESTAR CRÓNICO?
  + **P408_3** - ¿TIENE UD LIMITACIONES DE FORMA PERMANENTE, PARA: Hablar o comunicarse, aun usando lenguaje de señas u otro?
  + **P408_6** - ¿TIENE UD LIMITACIONES DE FORMA PERMANENTE, PARA: Relacionarse con los demás, por sus pensamientos, sentimientos.
  + **NOMBDEPA** - Departamento
  + **P501** - Nivel de instruccion
  + **P601** - LA SEMANA PASADA, ¿TUVO UD. ALGÚN TRABAJO? (sin contar con los quehaceres del hogar)?
  + **P627** - EN VENEZUELA, ¿TENÍA USTED TRABAJO ANTES DE INICIAR SU VIAJE?
  + **P706** - EN VENEZUELA, ¿USTED PARTICIPABA DE ASOCIACIONES / ESPACIOS DE REUNIÓN COMUNITARIOS?
  + **P807** - DESDE QUE LLEGÓ AL PERÚ, ¿SABE O CONOCE DE ALGUNA PERSONA VENEZOLANA QUE HA SIDO VÍCTIMA DE MALTRATO VERBAL?
  
 
Se usaran entonces 12 variables predictores para predecir si una persona de nacionalidad
venezolana que reside en el Peru ha sido o no discriminada

Descartando variables para quedarnos solo con las seleccionadas para la prediccion

```{r}
datos$Dia <- NULL
datos$Mes <- NULL
datos$Anio <- NULL
datos$P304 <- NULL
datos$TIENE_SEGURO <- NULL
datos$P408_1 <- NULL
datos$P408_2 <- NULL
datos$NOMBPROV <- NULL
datos$NOMBDIST <- NULL
datos$P504 <- NULL
datos$P609 <- NULL
datos$P703 <- NULL
datos$Sexo <- NULL
datos$Migro_de_venezuela <- NULL

str(datos)

```

### Verificando si el conjunto de datos esta balanceado
```{r}
round(prop.table(table(datos$P701))*100,2)

```

Los datos siguen un ratio aproximado de 30:40 por lo tanto no es necesario hacer balanceo

### Explorando datos perdidos
```{r}
library(DataExplorer)
plot_missing(datos,theme= theme_bw())
```
### Dividir el conjunto de datos en training y testing
Se divide la data en 80% training y 20% testing
```{r}
library(caret)
set.seed(2021) 
index    <- createDataPartition(datos$P701, 
                                p=0.8, 
                                list=FALSE)

training <- datos[ index, ]            # 943 datos trainig             
testing  <- datos[-index, ]            # 402 datos testing

# Verificando la proporcion del target en datos particionados
prop.table(table(datos$P701))
prop.table(table(training$P701))
prop.table(table(testing$P701))
```
### Pre procesamiento de datos
```{r}
library(recipes)
set.seed(2021)
trained_recipe <- recipe(P701 ~ .,
                         data =  training) %>%
  step_knnimpute(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_range(all_numeric()) %>%   # Min-Max [0,1]
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_other(all_nominal(), -all_outcomes(), threshold = 0.07, other = "other") %>%
  prep()

data.train <- bake(trained_recipe, new_data = NULL)
data.test  <- bake(trained_recipe, new_data = testing)

data.train <- as.data.frame(data.train)
data.test  <- as.data.frame(data.test)
```


Seleccionando las variables mas importantes con el algoritmo BORUTO
```{r}
library(Boruta)
set.seed(2021)

boruta.data <- Boruta(P701 ~ ., data = data.train, doTrace = 2)

print(boruta.data)

plot(boruta.data,cex.axis=0.5)

plotImpHistory(boruta.data,lty=1)

final.boruta.bank <- TentativeRoughFix(boruta.data)
print(final.boruta.bank)

getSelectedAttributes(final.boruta.bank, withTentative = F)

boruta.df <- attStats(final.boruta.bank)
print(boruta.df)
boruta.df[order(-boruta.df$meanImp),]
```

Luego de aplicar el algoritmo, el resultado obtenido nos dice que solo 11 variables con consideradas importantes, por lo que las restantes
se eliminar de el conjunto de datos de training y tambien de testing al ya no considerarse relevantes para la prediccion

```{r}
data.train$P501_Educacion.Media.Diversificada.Incompleta <- NULL
data.train$P501_Preescolar <- NULL
data.train$NOMBDEPA_LA.LIBERTAD <- NULL
data.train$P501_Superior.Universitaria.Completa <- NULL
data.train$NOMBDEPA_TUMBES <- NULL
data.train$NOMBDEPA_CALLAO <- NULL
data.train$NOMBDEPA_LIMA <- NULL
data.train$P501_Superior.Universitaria.Incompleta <- NULL
data.train$P501_Tecnico.Superior.Incompleta <- NULL
data.train$P501_Maestria..Doctorado <- NULL
data.train$P501_Tecnico.Superior.Completa <- NULL
data.train$P501_Educacion.Basica.Completa <- NULL

data.test$P501_Educacion.Media.Diversificada.Incompleta <- NULL
data.test$P501_Preescolar <- NULL
data.test$NOMBDEPA_LA.LIBERTAD <- NULL
data.test$P501_Superior.Universitaria.Completa <- NULL
data.test$NOMBDEPA_TUMBES <- NULL
data.test$NOMBDEPA_CALLAO <- NULL
data.test$NOMBDEPA_LIMA <- NULL
data.test$P501_Superior.Universitaria.Incompleta <- NULL
data.test$P501_Tecnico.Superior.Incompleta <- NULL
data.test$P501_Maestria..Doctorado <- NULL
data.test$P501_Tecnico.Superior.Completa <- NULL
data.test$P501_Educacion.Basica.Completa <- NULL

str(data.train)
str(data.test)
```


Verificacion del conjunto de datos
```{r}
str(data.train)

plot_missing(data.train,theme= theme_bw())
plot_missing(data.test,theme= theme_bw())
```

### Etapa de entrenamiento de modelos

Configuracion de cross validation con 3 repeticiones y 10 folds
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats = 3, number=10, classProbs =  TRUE)
```

Definiendo la variable target y las variables predictoras
```{r}
target     <- "P701"
predictores <- setdiff(names(data.train), target)
predictores
```

### Modelo KNN 

```{r}
set.seed(2021)
modelo_knn <- train(data.train[,predictores], 
                    data.train[,target],
                    method = "knn",
                    trControl = ctrl, 
                    #tuneLength = 5, # Al azar 5 valores de k
                    tuneGrid = expand.grid(k=seq(1,91,2)),
                    metric="Accuracy")

modelo_knn

modelo_knn$finalModel

modelo_knn$bestTune

plot(modelo_knn)
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.KNN <- predict(modelo_knn, newdata = data.test, type="prob")
PROBA.KNN <- PROBA.KNN[,2]
CLASE.KNN <- predict(modelo_knn, newdata = data.test)

# Matriz de confusion
cm_knn <- caret::confusionMatrix(CLASE.KNN,
                                 data.test$P701,
                                 positive="Si")

cm_knn$table

# Area bajo la curva
colAUC(PROBA.KNN, data.test$P701, plotROC = TRUE) -> auc_knn
abline(0, 1,col="red")
auc_knn

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.KNN,real) -> logloss_knn
logloss_knn

```

### Modelo Regresion logistica

```{r}
set.seed(2021)
modelo_rl <- train(data.train[,predictores], 
                    data.train[,target],
                    method = "glm",
                    family="binomial",
                    trControl = ctrl, 
                    tuneLength = 5,
                    metric="Accuracy")

modelo_rl
modelo_rl$finalModel
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.RL <- predict(modelo_rl, newdata = data.test, type="prob")
PROBA.RL <- PROBA.RL[,2]
CLASE.RL <- predict(modelo_rl, newdata = data.test)

# Matriz de confusion
cm_rl <- caret::confusionMatrix(CLASE.RL,
                                 data.test$P701,
                                 positive="Si")

cm_rl$table

# Area bajo la curva
colAUC(PROBA.RL, data.test$P701, plotROC = TRUE) -> auc_rl
abline(0, 1,col="red")
auc_rl

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.RL,real) -> logloss_rl
logloss_rl
```

### Modelo naive-bayes

```{r}
set.seed(2021)
modelo_nb <- train(data.train[,predictores], 
                   data.train[,target],
                   method = "nb", 
                   trControl = ctrl, 
                   tuneLength = 5,
                   metric="Accuracy" )

modelo_nb 
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.NB <- predict(modelo_nb, newdata = data.test, type="prob")
PROBA.NB <- PROBA.NB[,2]
CLASE.NB <- predict(modelo_nb, newdata = data.test)

# Matriz de confusion
cm_nb <- caret::confusionMatrix(CLASE.NB,
                                 data.test$P701,
                                 positive="Si")

cm_nb$table

# Area bajo la curva
colAUC(PROBA.NB, data.test$P701, plotROC = TRUE) -> auc_nb
abline(0, 1,col="red")
auc_nb

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.NB,real) -> logloss_nb
logloss_nb
```



### Modelo SVM Lineal (SVC) 

```{r}
set.seed(2021)
modelo_svc <- train(P701 ~ ., 
                    data = data.train,
                    method = "svmLinear",
                    trControl = ctrl, 
                    tuneLength = 5, 
                    #tuneGrid = expand.grid(C = seq(0,2,length= 20)),
                    metric="Accuracy")

modelo_svc
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.SVC <- predict(modelo_svc, newdata= data.test, type="prob")
PROBA.SVC <- PROBA.SVC[,2]
CLASE.SVC <- predict(modelo_svc, newdata = data.test)

# Matriz de confusion
cm_svc <- caret::confusionMatrix(CLASE.SVC,
                                 data.test$P701,
                                 positive="Si")

cm_svc$table

# Area bajo la curva
colAUC(PROBA.SVC, data.test$P701, plotROC = TRUE) -> auc_svc
abline(0, 1,col="red")
auc_svc

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.SVC,real) -> logloss_svc
logloss_svc
```



### Modelo SVM (Radial o polinomial)

```{r}
# Modelo SVM
set.seed(2021)
modelo_svm <- train(P701 ~ ., 
                 data = data.train, 
                 method = "svmRadial", 
                 trControl = ctrl, 
                 #tunelength = 10,
                 tuneGrid = expand.grid(C=seq(1,2,length=10),
                                        sigma=0.05317),
                 metric="Accuracy")


modelo_svm

modelo_svm$bestTune
# Los mejor hiper parametros encontrados durante el entrenamiento son
# sigma = 0.05317 y C = 1.333
# Estos se obtuvieron durante una segunda pasada de entrenamiento usando tuneGrid
# y considerando como sigma al mejor sigma obtenido de la primera pasada

plot(modelo_svm)


PROBA.SVM <- predict(modelo_svm, newdata= data.test, type="prob")
PROBA.SVM <- PROBA.SVM[,2]

CLASE.SVM <- predict(modelo_svm,newdata = data.test) # 0.5
head(CLASE.SVM)

# Evaluando la performance del modelo SVM ---------------------

# Calcular el accuracy
accuracy_svm <- mean(data.test$P701==CLASE.SVM);accuracy_svm

# Matriz de Confusion usando el paquete caret
library(caret)
cm_svm <- caret::confusionMatrix(CLASE.SVM,
                                 data.test$P701,
                                 positive="Si")
cm_svm

cm_svm$table

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.SVM,data.test$P701,plotROC = TRUE) -> auc_svm
abline(0, 1,col="red")
auc_svm

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.SVM,real) -> logloss_svm
logloss_svm

```

```{r}

```



### Red neuronal 

```{r}
# 3.3 Red Neuronal con Validación Cruzada Repetida ------------
# Relación de parámetros a ajustar de un modelo
modelLookup(model='nnet')

set.seed(123)
modelo3 <- train(P701 ~ ., 
                 data = data.train, 
                 method = "nnet",
                 trControl = ctrl, 
                 tuneLength = 5, 
                 #tuneGrid = expand.grid(size=seq(1,3,1),
                 #              decay=seq(0,0.05,0.005)),
                 metric="Accuracy")


# X1  X2  Y
# 20  3  Fuga
# 20  3  No Fuga

modelo3

modelo3$bestTune

plot(modelo3)

plotnet(modelo3$finalModel)

# Importancia de las variables
olden(modelo3)
garson(modelo3)

varImp(modelo3)
plot(varImp(modelo3))

PROBA.RNA <- predict(modelo3,newdata = data.test, type="prob")
PROBA.RNA <- PROBA.RNA[,2]
head(PROBA.RNA)

CLASE.RNA <- predict(modelo3,newdata = data.test ) # unbral = 0.5
head(CLASE.RNA)

# Evaluando la performance de la Red Neuronal -----------------
# Matriz de Confusión
tabla3  <- table(Predicho=CLASE.RNA,Real= data.test$P701)
addmargins(tabla3)

library(yardstick)
autoplot(conf_mat(tabla3),type="heatmap")  
summary(conf_mat(tabla3), event_level = "second") 

# Calcular el accuracy
accuracy_rna <- mean(data.test$P701==CLASE.RNA) ; accuracy_rna

# Calcular el error de mala clasificación
error <- mean(data.test$P701!=CLASE.RNA) ; error

# Matriz de Confusión usando el paquete caret
library(caret)
cm_rna <- caret::confusionMatrix(CLASE.RNA,
                                 data.test$P701,
                                 positive="Si")
cm_rna

cm_rna$table

cm_rna$byClass["Sensitivity"] 
cm_rna$byClass["Specificity"] 
cm_rna$overall["Accuracy"]

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.RNA,data.test$P701,plotROC = TRUE) -> auc_rna
abline(0, 1,col="red",lty=2)
auc_rna

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.RNA,real) -> logloss_rna
logloss_rna
```

```{r}

```



### Modelo C5

```{r}
# Modelo Arbol C5

set.seed(2021)
modelo_C5    <- train(P701 ~ ., 
                      data = data.train, 
                      method = "C5.0", 
                      trControl = ctrl, 
                      tuneLength = 5,
                      metric="Accuracy")

modelo_C5

modelo_C5$bestTune

plot(modelo_C5)


# Predicción de la clase y probabilidad 
PROBA.C5 <- predict(modelo_C5,newdata = data.test,type="prob")
head(PROBA.C5)
PROBA.C5 <- PROBA.C5[,2]

CLASE.C5 <- predict(modelo_C5,newdata = data.test )
head(CLASE.C5)

# Evaluando la performance del modelo C5.0 --------------------
# Matriz de Confusion
library(gmodels)
CrossTable(x = data.test$P701, y = CLASE.C5,
           prop.t=FALSE, prop.c=FALSE, prop.chisq = FALSE)

addmargins(table(Real=data.test$P701,Clase_Predicha=CLASE.C5))
prop.table(table(Real=data.test$P701,Clase_Predicha=CLASE.C5),1)

# Calcular el accuracy
accuracy_c5 <- mean(data.test$P701==CLASE.C5) ; accuracy_c5

# Calcular el error de mala clasificación
error <- mean(data.test$P701!=CLASE.C5) ; error

cm_c5 <- caret::confusionMatrix(CLASE.C5,
                                data.test$P701,
                                positive="Si")

cm_c5
cm_c5$byClass["Sensitivity"] 
cm_c5$byClass["Specificity"] 
cm_c5$overall["Accuracy"]

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.C5,data.test$P701,plotROC = TRUE) -> auc_c5
abline(0, 1,col="red")
auc_c5

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.C5,real) -> logloss_c5
logloss_c5
```

```{r}

```



### Modelo CART

```{r}
# Modelo CART
set.seed(2021)
modelo_cart <- train(P701 ~ ., 
                     data = data.train, 
                     method = "rpart", 
                     trControl = ctrl, 
                     tuneLength = 20,
                     #tuneGrid = expand.grid(cp=seq(0,0.5,0.001)),
                     metric="Accuracy")

modelo_cart

#        cp  Accuracy
#  0.002316    0.7188

modelo_cart$bestTune
plot(modelo_cart)


modelo_cart$finalModel


PROBA.CART <- predict(modelo_cart,
                      newdata = data.test, 
                      type="prob")
PROBA.CART <- PROBA.CART[,2]
head(PROBA.CART)

CLASE.CART <- predict(modelo_cart,newdata = data.test )
head(CLASE.CART)


# Evaluando la performance del modelo CART --------------------
# Matriz de Confusion
library(gmodels)
CrossTable(x = data.test$P701, y = CLASE.CART,
           prop.t=FALSE, prop.c=FALSE, prop.chisq = FALSE)

addmargins(table(Real=data.test$P701,Clase_Predicha=CLASE.CART))
prop.table(table(Real=data.test$P701,Clase_Predicha=CLASE.CART),1)

# Calcular el accuracy
accuracy_cart <- mean(data.test$P701==CLASE.CART) ; accuracy_cart

# Calcular el error de mala clasificación
error <- mean(data.test$P701!=CLASE.CART) ; error

cm_cart <- caret::confusionMatrix(CLASE.CART,
                                  data.test$P701,
                                  positive="Si")

cm_cart
cm_cart$byClass["Sensitivity"] 
cm_cart$byClass["Specificity"] 
cm_cart$overall["Accuracy"]

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.CART,data.test$P701,plotROC = TRUE) -> auc_cart
abline(0, 1,col="red")
auc_cart

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.CART,real) -> logloss_cart
logloss_cart
```

```{r}

```



### Modelo Bagging

```{r}
set.seed(2021)
modelo_bag <- train( ~ ., 
                    data = data.train, 
                    method = "treebag",
                    trControl = ctrl, 
                    #tuneLength = 5, 
                    metric="Accuracy")

modelo_bag

# treebag no tiene hiperparÃ¡metros para hacer tunning
plot(modelo_bag)

varImp(modelo_bag)

plot(varImp(modelo_bag))

PROBA.BAG <- predict(modelo_bag,newdata = data.test, type="prob")
PROBA.BAG <- PROBA.BAG[,2]  # Columna 1 = Probabilidad Actual(0)  
head(PROBA.BAG)             # Columna 2 = Probabilidad   Fuga(1)  

CLASE.BAG <- predict(modelo_bag,newdata = data.test )
head(CLASE.BAG)

head(cbind(data.test,PROBA.BAG,CLASE.BAG))

# Evaluando la performance del modelo Bagging -----------------
# Matriz de ConfusiÃ³n
library(gmodels)
CrossTable(x = data.test$CHURN, y = CLASE.BAG,
           prop.t=FALSE, prop.c=FALSE, prop.chisq = FALSE)

addmargins(table(Real=data.test$CHURN,Clase_Predicha=CLASE.BAG))
prop.table(table(Real=data.test$CHURN,Clase_Predicha=CLASE.BAG),1)

# Calcular el accuracy
accuracy_bag <- mean(data.test$CHURN==CLASE.BAG) ; accuracy_bag

# Calcular el error de mala clasificaciÃ³n
error <- mean(data.test$CHURN!=CLASE.BAG) ; error

cm_bag <- caret::confusionMatrix(CLASE.BAG,
                                 data.test$CHURN,
                                 positive="Fuga")

cm_bag
cm_bag$byClass["Sensitivity"] 
cm_bag$byClass["Specificity"] 
cm_bag$overall["Accuracy"]

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.BAG,data.test$CHURN,plotROC = TRUE) -> auc_bag
abline(0, 1,col="red")
auc_bag

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$CHURN)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.BAG,real) -> logloss_bag
logloss_bag
```

```{r}

```



### Modelo Random Forest 

```{r}

```

```{r}

```



### Modelo AdaBoosting

```{r}

```

```{r}

```



### Modelo GBM (GRADIENT BOOSTING MACHINE)

```{r}
set.seed(2021)
modelo_gbm <- train(P701 ~ ., data = data.train, 
                    method = "gbm", 
                    tuneLength=2,
                    #tuneGrid = expand.grid(n.trees=c(50,100,150),
                    #                       interaction.depth=c(1,2,3),
                    #                       shrinkage=0.1,
                    #                       n.minobsinnode=10),
                    trControl=ctrl, metric="Accuracy")

modelo_gbm

modelo_gbm$bestTune

plot(modelo_gbm)

ggplot(modelo_gbm, highlight = T) +
  labs(title = "Evolución del accuracy con GBM") +
  guides(color = guide_legend(title = "Max Tree Depth"),
         shape = guide_legend(title = "Max Tree Depth")) +
  theme_bw() +
  theme(legend.position = "bottom")

varImp(modelo_gbm)

plot(varImp(modelo_gbm))

PROBA.GBM <- predict(modelo_gbm,newdata = data.test, type="prob")
PROBA.GBM <- PROBA.GBM[,2]
head(PROBA.GBM)

CLASE.GBM <- predict(modelo_gbm,newdata = data.test )
head(CLASE.GBM)

head(cbind(data.test,PROBA.GBM,CLASE.GBM))

# Evaluando la performance del modelo GBM ---------------------
# Matriz de Confusión
library(gmodels)
CrossTable(x = data.test$P701, y = CLASE.GBM,
           prop.t=FALSE, prop.c=FALSE, prop.chisq = FALSE)

addmargins(table(Real=data.test$P701,Clase_Predicha=CLASE.GBM))
prop.table(table(Real=data.test$P701,Clase_Predicha=CLASE.GBM),1)

# Calcular el accuracy
accuracy_gbm <- mean(data.test$P701==CLASE.GBM) ; accuracy_gbm

# Calcular el error de mala clasificación
error <- mean(data.test$P701!=CLASE.GBM) ; error

cm_gbm <- caret::confusionMatrix(CLASE.GBM,
                                 data.test$P701,
                                 positive="Si")

cm_gbm
cm_gbm$byClass["Sensitivity"] 
cm_gbm$byClass["Specificity"] 
cm_gbm$overall["Accuracy"]

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.GBM,data.test$P701,plotROC = TRUE) -> auc_gbm
abline(0, 1,col="red")
auc_gbm

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.GBM,real) -> logloss_gbm
logloss_gbm
```

```{r}

```



### Modelo XGBoosting

```{r}
set.seed(2021)
modelo_xg <- train(P701 ~ ., 
                   data = data.train, 
                   method = "xgbTree", 
                   tuneLength=2,
                   trControl=ctrl, 
                   metric="Accuracy")

modelo_xg

modelo_xg$bestTune

modelo_xg$resample

plot(modelo_xg)

varImp(modelo_xg)

plot(varImp(modelo_xg))

library(xgboost)
library(DiagrammeR)
xgb.plot.tree(model = modelo_xg$finalModel, trees = 1)
xgb.plot.tree(model = modelo_xg$finalModel, trees = 5)

PROBA.XG <- predict(modelo_xg,newdata = data.test, type="prob")
PROBA.XG <- PROBA.XG[,2]
head(PROBA.XG)

CLASE.XG <- predict(modelo_xg,newdata = data.test )
head(CLASE.XG)

head(cbind(data.test,PROBA.XG,CLASE.XG))

# Evaluando la performance del modelo XGBoosting --------------
# Matriz de Confusión
library(gmodels)
CrossTable(x = data.test$P701, y = CLASE.XG,
           prop.t=FALSE, prop.c=FALSE, prop.chisq = FALSE)

addmargins(table(Real=data.test$P701,Clase_Predicha=CLASE.XG))
prop.table(table(Real=data.test$P701,Clase_Predicha=CLASE.XG),1)

# Calcular el accuracy
accuracy_xg <- mean(data.test$P701==CLASE.XG) ; accuracy_xg

# Calcular el error de mala clasificación
error <- mean(data.test$P701!=CLASE.XG) ; error

cm_xg <- caret::confusionMatrix(CLASE.XG,
                                data.test$P701,
                                positive="Si")

cm_xg
cm_xg$byClass["Sensitivity"] 
cm_xg$byClass["Specificity"] 
cm_xg$overall["Accuracy"]

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.XG,data.test$P701,plotROC = TRUE) -> auc_xg
abline(0, 1,col="red")
auc_xg

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.XG,real) -> logloss_xg
logloss_xg
```

```{r}

```