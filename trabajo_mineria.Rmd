---
title: "Trabajo final mineria"
output:
  html_notebook: default
  pdf_document: default
---

Limpiar el workspace, la consola y cambiar el directorio de trabajo
```{r}
rm(list = ls())
graphics.off()
cat("\014")
options(scipen=999)
options(digits = 3)
# cambiar directiorio
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
```
Se cargan los paquetes
```{r}
library(pacman)
p_load(foreign, DataExplorer, recipes, MLmetrics, caret, 
       caTools, yardstick, funModeling, DataExplorer, VIM, NeuralNetTools, fastAdaboost,
       ggplot2, randomForest, adabag, caretEnsemble, pROC)

```


Cargar el conjunto de datos
```{r}
datos <- read.csv("enpove_encuesta.csv")
```

Conversion de las variables al tipo de dato apropiado
```{r}
str(datos)

# No considerar la variable de identificaciÃ³n ID
datos$CODIGO_PERSONA <- NULL 

# Etiquetando las opciones de las variables categÃ³ricas
datos$Dia <- as.numeric(datos$Dia)
datos$Mes <- as.numeric(datos$Mes)
datos$Anio <- as.numeric(datos$Anio)

datos$P304 <- as.factor(datos$P304)
datos$P311 <- as.factor(datos$P311)
datos$P313 <- as.factor(datos$P313)
datos$TIENE_SEGURO <- as.factor(datos$TIENE_SEGURO)
datos$P402 <- as.factor(datos$P402)

datos$P408_1 <- as.factor(datos$P408_1)
datos$P408_2 <- as.factor(datos$P408_2)
datos$P408_3 <- as.factor(datos$P408_3)
datos$P408_6 <- as.factor(datos$P408_6)

datos$NOMBDEPA <- as.factor(datos$NOMBDEPA)
datos$NOMBPROV <- as.factor(datos$NOMBPROV)
datos$NOMBDIST <- as.factor(datos$NOMBDIST)

datos$P501 <- as.factor(datos$P501)
datos$P504 <- as.factor(datos$P504)
datos$P601 <- as.factor(datos$P601)
datos$P609 <- as.factor(datos$P609)
datos$P627 <- as.factor(datos$P627)

#target
###################################
datos$P701 <- as.factor(datos$P701)
###################################

datos$P703 <- as.factor(datos$P703)
datos$P706 <- as.factor(datos$P706)
datos$P807 <- as.factor(datos$P807)

datos$Sexo <- as.factor(datos$Sexo)
datos$Edad <- as.numeric(datos$Edad)
datos$Migro_de_venezuela <- as.factor(datos$Migro_de_venezuela)


str(datos)

```
Colocar los nombres a las categorias
```{r}
datos$Dia <- as.numeric(datos$Dia)
datos$Mes <- as.numeric(datos$Mes)
datos$Anio <- as.numeric(datos$Anio)

levels(datos$P304) <- c("Si", "No")
levels(datos$P311) <- c("Si", "No")
levels(datos$P313) <- c("Si", "No")
levels(datos$TIENE_SEGURO) <- c("1", "2", "3", "5")
levels(datos$P402) <- c("Si", "No")

levels(datos$P408_1) <- c("Si", "No")
levels(datos$P408_2) <- c("Si", "No")
levels(datos$P408_3) <- c("Si", "No")
levels(datos$P408_6) <- c("Si", "No")

levels(datos$P501) <- c("Sin nivel", "Preescolar", "Educacion Básica Incompleta", "Educacion Basica Completa", "Educacion Media Diversificada Incompleta", "Educacion Media Diversificada Completa", "Tecnico Superior Incompleta", "Tecnico Superior Completa", "Superior Universitaria Incompleta", "Superior Universitaria Completa", "Maestria/ Doctorado")
levels(datos$P504) <- c("Si", "No")
levels(datos$P601) <- c("Si", "No")
levels(datos$P609) <- c("Si", "No")
levels(datos$P627) <- c("Si", "No")

#target
###################################
levels(datos$P701) <- c("Si", "No")
###################################

levels(datos$P703) <- c("Si", "No")
levels(datos$P706) <- c("Si", "No")
levels(datos$P807) <- c("Si", "No")

levels(datos$Sexo) <- c("Hombre", "Mujer")
levels(datos$Migro_de_venezuela) <- c("Si", "No")

str(datos)
```
Analisis de variables
```{r}
tapply(datos$Dia, datos$P701, mean)

# 15.4 15.6

plotar(datos, 
       input = "Dia", 
       target="P701", 
       plot_type="histdens")

```
```{r}
tapply(datos$Mes, datos$P701, mean)

#   Si   No 
#  6.74 6.75

plotar(datos, 
       input = "Mes", 
       target="P701", 
       plot_type="histdens")
```

```{r}
tapply(datos$Anio, datos$P701, mean)

#  Si   No 
# 1989 1989

plotar(datos, 
       input = "Anio", 
       target="P701", 
       plot_type="histdens")
```
Las variables Dia, mes y anio no van en el modelo


```{r}
datos$NOMBDIST <- NULL

dependiente <- "P701"
solo_cat <- lapply(Filter(is.factor, datos), levels)
predictores <- setdiff(names(solo_cat), dependiente)
predictores

cross_plot(datos, 
           input     = predictores, 
           target    = dependiente,
           plot_type = "percentual") 
```

```{r}
tapply(datos$Edad, datos$P701, mean)

#  Si   No 
# 29.3 28.6 

plotar(datos, 
       input = "Edad", 
       target="P701", 
       plot_type="histdens")

```
Variables descartadas

* Anio
* Mes
* Dia
* P304
* TIENE_SEGURO
* P408_1
* P408_2
* NOMBPROV
* NOMBDIST
* P504
* P609
* P703
* Sexo
* Migro_venezuela


Variables seleccionadas

* Numericas 
  + **Edad** - Edad del entrevistado
* Categoricas
  + **P311** - DESDE QUE LLEGÓ AL PERÚ. ¿HA VIVIDO SIEMPRE EN ESTE DISTRITO?
  + **P313** - USTED, ¿PIENSA QUEDARSE A VIVIR EN PERÚ?
  + **P402** - ¿PADECE DE ALGUNA ENFERMEDAD O MALESTAR CRÓNICO?
  + **P408_3** - ¿TIENE UD LIMITACIONES DE FORMA PERMANENTE, PARA: Hablar o comunicarse, aun usando lenguaje de señas u otro?
  + **P408_6** - ¿TIENE UD LIMITACIONES DE FORMA PERMANENTE, PARA: Relacionarse con los demás, por sus pensamientos, sentimientos.
  + **NOMBDEPA** - Departamento
  + **P501** - Nivel de instruccion
  + **P601** - LA SEMANA PASADA, ¿TUVO UD. ALGÚN TRABAJO? (sin contar con los quehaceres del hogar)?
  + **P627** - EN VENEZUELA, ¿TENÍA USTED TRABAJO ANTES DE INICIAR SU VIAJE?
  + **P706** - EN VENEZUELA, ¿USTED PARTICIPABA DE ASOCIACIONES / ESPACIOS DE REUNIÓN COMUNITARIOS?
  + **P807** - DESDE QUE LLEGÓ AL PERÚ, ¿SABE O CONOCE DE ALGUNA PERSONA VENEZOLANA QUE HA SIDO VÍCTIMA DE MALTRATO VERBAL?
  
 
Se usaran entonces 12 variables predictores para predecir si una persona de nacionalidad
venezolana que reside en el Peru ha sido o no discriminada

Descartando variables para quedarnos solo con las seleccionadas para la prediccion

```{r}
datos$Dia <- NULL
datos$Mes <- NULL
datos$Anio <- NULL
datos$P304 <- NULL
datos$TIENE_SEGURO <- NULL
datos$P408_1 <- NULL
datos$P408_2 <- NULL
datos$NOMBPROV <- NULL
datos$NOMBDIST <- NULL
datos$P504 <- NULL
datos$P609 <- NULL
datos$P703 <- NULL
datos$Sexo <- NULL
datos$Migro_de_venezuela <- NULL

str(datos)

```

### Verificando si el conjunto de datos esta balanceado
```{r}
round(prop.table(table(datos$P701))*100,2)

```

Los datos siguen un ratio aproximado de 30:40 por lo tanto no es necesario hacer balanceo

### Explorando datos perdidos
```{r}
library(DataExplorer)
plot_missing(datos,theme= theme_bw())
```
### Dividir el conjunto de datos en training y testing
Se divide la data en 80% training y 20% testing
```{r}
library(caret)
set.seed(2021) 
index    <- createDataPartition(datos$P701, 
                                p=0.8, 
                                list=FALSE)

training <- datos[ index, ]            # 943 datos trainig             
testing  <- datos[-index, ]            # 402 datos testing

# Verificando la proporcion del target en datos particionados
prop.table(table(datos$P701))
prop.table(table(training$P701))
prop.table(table(testing$P701))
```

### Pre procesamiento de datos
```{r}
library(recipes)
set.seed(2021)
trained_recipe <- recipe(P701 ~ .,
                         data =  training) %>%
  step_knnimpute(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_range(all_numeric()) %>%   # Min-Max [0,1]
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_other(all_nominal(), -all_outcomes(), threshold = 0.07, other = "other") %>%
  prep()

data.train <- bake(trained_recipe, new_data = NULL)
data.test  <- bake(trained_recipe, new_data = testing)

data.train <- as.data.frame(data.train)
data.test  <- as.data.frame(data.test)
```


Seleccionando las variables mas importantes con el algoritmo BORUTO
```{r}
library(Boruta)
set.seed(2021)

boruta.data <- Boruta(P701 ~ ., data = data.train, doTrace = 2)

print(boruta.data)

plot(boruta.data,cex.axis=0.5)

plotImpHistory(boruta.data,lty=1)

final.boruta.bank <- TentativeRoughFix(boruta.data)
print(final.boruta.bank)

getSelectedAttributes(final.boruta.bank, withTentative = F)

boruta.df <- attStats(final.boruta.bank)
print(boruta.df)
boruta.df[order(-boruta.df$meanImp),]
```

Luego de aplicar el algoritmo, el resultado obtenido nos dice que solo 11 variables con consideradas importantes, por lo que las restantes
se eliminar de el conjunto de datos de training y tambien de testing al ya no considerarse relevantes para la prediccion

```{r}
data.train$P501_Educacion.Media.Diversificada.Incompleta <- NULL
data.train$P501_Preescolar <- NULL
data.train$NOMBDEPA_LA.LIBERTAD <- NULL
data.train$P501_Superior.Universitaria.Completa <- NULL
data.train$NOMBDEPA_TUMBES <- NULL
data.train$NOMBDEPA_CALLAO <- NULL
data.train$NOMBDEPA_LIMA <- NULL
data.train$P501_Superior.Universitaria.Incompleta <- NULL
data.train$P501_Tecnico.Superior.Incompleta <- NULL
data.train$P501_Maestria..Doctorado <- NULL
data.train$P501_Tecnico.Superior.Completa <- NULL
data.train$P501_Educacion.Basica.Completa <- NULL

data.test$P501_Educacion.Media.Diversificada.Incompleta <- NULL
data.test$P501_Preescolar <- NULL
data.test$NOMBDEPA_LA.LIBERTAD <- NULL
data.test$P501_Superior.Universitaria.Completa <- NULL
data.test$NOMBDEPA_TUMBES <- NULL
data.test$NOMBDEPA_CALLAO <- NULL
data.test$NOMBDEPA_LIMA <- NULL
data.test$P501_Superior.Universitaria.Incompleta <- NULL
data.test$P501_Tecnico.Superior.Incompleta <- NULL
data.test$P501_Maestria..Doctorado <- NULL
data.test$P501_Tecnico.Superior.Completa <- NULL
data.test$P501_Educacion.Basica.Completa <- NULL

str(data.train)
str(data.test)
```


Verificacion del conjunto de datos
```{r}
str(data.train)

plot_missing(data.train,theme= theme_bw())
plot_missing(data.test,theme= theme_bw())
```

### Etapa de entrenamiento de modelos

Configuracion de cross validation con 3 repeticiones y 10 folds
```{r}
ctrl <- trainControl(method="repeatedcv",
                     repeats = 3, number=10, savePredictions = 'final', classProbs =  TRUE)
```

Definiendo la variable target y las variables predictoras
```{r}
target     <- "P701"
predictores <- setdiff(names(data.train), target)
predictores
```

### Modelo KNN 

```{r}
set.seed(2021)
modelo_knn <- train(data.train[,predictores], 
                    data.train[,target],
                    method = "knn",
                    trControl = ctrl, 
                    #tuneLength = 5, # Al azar 5 valores de k
                    tuneGrid = expand.grid(k=seq(1,91,2)),
                    metric="Accuracy")

modelo_knn

modelo_knn$finalModel

modelo_knn$bestTune

plot(modelo_knn)
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.KNN <- predict(modelo_knn, newdata = data.test, type="prob")
PROBA.KNN <- PROBA.KNN[,2]
CLASE.KNN <- predict(modelo_knn, newdata = data.test)

# Calcular el accuracy
accuracy_knn <- mean(data.test$P701==CLASE.KNN);
accuracy_knn

# Matriz de confusion
cm_knn <- caret::confusionMatrix(CLASE.KNN,
                                 data.test$P701,
                                 positive="Si")

cm_knn$table

# Area bajo la curva
colAUC(PROBA.KNN, data.test$P701, plotROC = TRUE) -> auc_knn
abline(0, 1,col="red")
auc_knn

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.KNN,real) -> logloss_knn
logloss_knn

```

### Modelo Regresion logistica

```{r}
set.seed(2021)
modelo_rl <- train(data.train[,predictores], 
                    data.train[,target],
                    method = "glm",
                    family="binomial",
                    trControl = ctrl, 
                    tuneLength = 5,
                    metric="Accuracy")

modelo_rl
modelo_rl$finalModel
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.RL <- predict(modelo_rl, newdata = data.test, type="prob")
PROBA.RL <- PROBA.RL[,2]
CLASE.RL <- predict(modelo_rl, newdata = data.test)

# Calcular el accuracy
accuracy_rl <- mean(data.test$P701==CLASE.RL);
accuracy_rl

# Matriz de confusion
cm_rl <- caret::confusionMatrix(CLASE.RL,
                                 data.test$P701,
                                 positive="Si")

cm_rl$table

# Area bajo la curva
colAUC(PROBA.RL, data.test$P701, plotROC = TRUE) -> auc_rl
abline(0, 1,col="red")
auc_rl

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.RL,real) -> logloss_rl
logloss_rl
```

### Modelo naive-bayes

```{r}
set.seed(2021)
modelo_nb <- train(data.train[,predictores], 
                   data.train[,target],
                   method = "nb", 
                   trControl = ctrl, 
                   tuneLength = 5,
                   metric="Accuracy" )

modelo_nb 
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.NB <- predict(modelo_nb, newdata = data.test, type="prob")
PROBA.NB <- PROBA.NB[,2]
CLASE.NB <- predict(modelo_nb, newdata = data.test)

# Calcular el accuracy
accuracy_nb <- mean(data.test$P701==CLASE.NB);
accuracy_nb

# Matriz de confusion
cm_nb <- caret::confusionMatrix(CLASE.NB,
                                 data.test$P701,
                                 positive="Si")

cm_nb$table

# Area bajo la curva
colAUC(PROBA.NB, data.test$P701, plotROC = TRUE) -> auc_nb
abline(0, 1,col="red")
auc_nb

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.NB,real) -> logloss_nb
logloss_nb
```



### Modelo SVM Lineal (SVC) 

```{r}
set.seed(2021)
modelo_svc <- train(P701 ~ ., 
                    data = data.train,
                    method = "svmLinear",
                    trControl = ctrl, 
                    tuneLength = 5, 
                    #tuneGrid = expand.grid(C = seq(0,2,length= 20)),
                    metric="Accuracy")

modelo_svc
```

Evaluando la performance del modelo
```{r}
set.seed(2021)

PROBA.SVC <- predict(modelo_svc, newdata= data.test, type="prob")
PROBA.SVC <- PROBA.SVC[,2]
CLASE.SVC <- predict(modelo_svc, newdata = data.test)


# Calcular el accuracy
accuracy_svc <- mean(data.test$P701==CLASE.SVC);
accuracy_svc

# Matriz de confusion
cm_svc <- caret::confusionMatrix(CLASE.SVC,
                                 data.test$P701,
                                 positive="Si")

cm_svc$table

# Area bajo la curva
colAUC(PROBA.SVC, data.test$P701, plotROC = TRUE) -> auc_svc
abline(0, 1,col="red")
auc_svc

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.SVC,real) -> logloss_svc
logloss_svc
```



### Modelo SVM (Radial o polinomial)

```{r}
# Modelo SVM
set.seed(2021)
modelo_svm <- train(P701 ~ ., 
                 data = data.train, 
                 method = "svmRadial", 
                 trControl = ctrl, 
                 #tunelength = 10,
                 tuneGrid = expand.grid(C=seq(1,2,length=10),
                                        sigma=0.05317),
                 metric="Accuracy")


modelo_svm

modelo_svm$bestTune

plot(modelo_svm)


PROBA.SVM <- predict(modelo_svm, newdata= data.test, type="prob")
PROBA.SVM <- PROBA.SVM[,2]
CLASE.SVM <- predict(modelo_svm,newdata = data.test) # 0.5


# Calcular el accuracy
accuracy_svm <- mean(data.test$P701==CLASE.SVM);
accuracy_svm

# Evaluando la performance del modelo SVM ---------------------

# Calcular el accuracy
accuracy_svm <- mean(data.test$P701==CLASE.SVM);accuracy_svm

# Matriz de Confusion usando el paquete caret
library(caret)
cm_svm <- caret::confusionMatrix(CLASE.SVM,
                                 data.test$P701,
                                 positive="Si")
cm_svm

cm_svm$table

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.SVM,data.test$P701,plotROC = TRUE) -> auc_svm
abline(0, 1,col="red")
auc_svm

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.SVM,real) -> logloss_svm

```



### Red neuronal 

```{r}

set.seed(2021)
modelo_rna <- train(P701 ~ ., 
                 data = data.train, 
                 method = "nnet",
                 trControl = ctrl, 
                 tuneLength = 5, 
                 #tuneGrid = expand.grid(size=seq(1,3,1),
                 #              decay=seq(0,0.05,0.005)),
                 metric="Accuracy")


modelo_rna
modelo_rna$bestTune

plot(modelo_rna)
plotnet(modelo_rna$finalModel)

PROBA.RNA <- predict(modelo_rna, newdata = data.test, type="prob")
PROBA.RNA <- PROBA.RNA[,2]
CLASE.RNA <- predict(modelo_rna, newdata = data.test )

# Calcular el accuracy
accuracy_rna <- mean(data.test$P701==CLASE.RNA);
accuracy_rna


library(caret)
cm_rna <- caret::confusionMatrix(CLASE.RNA,
                                 data.test$P701,
                                 positive="Si")
cm_rna
cm_rna$table

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.RNA, data.test$P701, plotROC = TRUE) -> auc_rna
abline(0, 1,col="red",lty=2)

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.RNA,real) -> logloss_rna
```


### Modelo C5

```{r}
# Modelo Arbol C5

set.seed(2021)
modelo_C5    <- train(P701 ~ ., 
                      data = data.train, 
                      method = "C5.0", 
                      trControl = ctrl, 
                      tuneLength = 5,
                      metric="Accuracy")

modelo_C5

modelo_C5$bestTune

plot(modelo_C5)


# Predicción de la clase y probabilidad 
PROBA.C5 <- predict(modelo_C5, newdata = data.test, type="prob")
PROBA.C5 <- PROBA.C5[,2]
CLASE.C5 <- predict(modelo_C5, newdata = data.test)


# Calcular el accuracy
accuracy_c5 <- mean(data.test$P701==CLASE.C5); 
accuracy_c5

cm_c5 <- caret::confusionMatrix(CLASE.C5,
                                data.test$P701,
                                positive="Si")

cm_c5

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.C5, data.test$P701, plotROC = TRUE) -> auc_c5
abline(0, 1,col="red")

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.C5, real) -> logloss_c5
```



### Modelo CART

```{r}
# Modelo CART
set.seed(2021)
modelo_cart <- train(P701 ~ ., 
                     data = data.train, 
                     method = "rpart", 
                     trControl = ctrl, 
                     tuneLength = 20,
                     #tuneGrid = expand.grid(cp=seq(0,0.5,0.001)),
                     metric="Accuracy")

modelo_cart
modelo_cart$bestTune

plot(modelo_cart)
modelo_cart$finalModel


PROBA.CART <- predict(modelo_cart,
                      newdata = data.test, 
                      type="prob")
PROBA.CART <- PROBA.CART[,2]
CLASE.CART <- predict(modelo_cart, newdata = data.test)


# Calcular el accuracy
accuracy_cart <- mean(data.test$P701==CLASE.CART); 
accuracy_cart

cm_cart <- caret::confusionMatrix(CLASE.CART,
                                  data.test$P701,
                                  positive="Si")

cm_cart

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.CART, data.test$P701, plotROC = TRUE) -> auc_cart
abline(0, 1,col="red")

# log loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.CART, real) -> logloss_cart
```



### Modelo Bagging

```{r}

set.seed(2021)
modelo_bag <- train(P701 ~ ., 
                    data = data.train, 
                    method = "treebag",
                    trControl = ctrl, 
                    #tuneLength = 5, 
                    metric="Accuracy")

modelo_bag

```

Evaluando la performance del modelo
```{r}

set.seed(2021)

PROBA.BAG <- predict(modelo_bag,newdata = data.test, type="prob")
PROBA.BAG <- PROBA.BAG[,2]
CLASE.BAG <- predict(modelo_bag,newdata = data.test )

# Calcular el accuracy
accuracy_bag <- mean(data.test$P701==CLASE.BAG);
accuracy_bag

cm_bag <- caret::confusionMatrix(CLASE.BAG,
                                 data.test$P701,
                                 positive="Si")

cm_bag

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.BAG,data.test$P701,plotROC = TRUE) -> auc_bag
abline(0, 1,col="red")

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.BAG,real) -> logloss_bag
```



### Modelo Random Forest 

```{r}

set.seed(2021)
modelo_rf <- train(P701 ~ ., 
                   data = data.train, 
                   method = "rf", 
                   trControl = ctrl, 
                   tuneLength = 5,
                   metric="Accuracy")
modelo_rf
```

Evaluando la perfomance del modelo
```{r}

set.seed(2021)
PROBA.RF <- predict(modelo_rf,newdata = data.test, type="prob")
PROBA.RF <- PROBA.RF[,2]
CLASE.RF <- predict(modelo_rf,newdata = data.test )

# Calcular el accuracy
accuracy_rf <- mean(data.test$P701==CLASE.RF);
accuracy_rf

cm_rf <- caret::confusionMatrix(CLASE.RF,
                                data.test$P701,
                                positive="Si")

cm_rf

# Area bajo la curva
library(caTools)
colAUC(PROBA.RF,data.test$P701, plotROC = TRUE) -> auc_rf
abline(0, 1,col="red")

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.RF,real) -> logloss_rf
```



### Modelo AdaBoosting

```{r}
set.seed(2021)
modelo_ada <- train(factor(P701) ~ ., 
                    data = data.train, 
                    method = "adaboost", 
                    tuneLength=2,
                    #tuneGrid = expand.grid(nIter=c(50,100),
                    #method=c("Breiman")),
                    trControl=ctrl, 
                    metric="Accuracy")

modelo_ada
modelo_ada$bestTune


PROBA.ADA <- predict(modelo_ada,newdata = data.test,type="prob")
PROBA.ADA <- PROBA.ADA[,2]
CLASE.ADA <- predict(modelo_ada,newdata = data.test )


# Calcular el accuracy
accuracy_ada <- mean(data.test$P701==CLASE.ADA);
accuracy_ada

cm_ada <- caret::confusionMatrix(CLASE.ADA,
                                 data.test$P701,
                                 positive="Si")

cm_ada

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.ADA, data.test$P701,plotROC = TRUE) -> auc_ada
abline(0, 1,col="red")

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.ADA,real) -> logloss_ada
```


### Modelo GBM (GRADIENT BOOSTING MACHINE)
```{r}
set.seed(2021)
modelo_gbm <- train(P701 ~ ., 
                    data = data.train, 
                    method = "gbm", 
                    tuneLength=2,
                    #tuneGrid = expand.grid(n.trees=c(50,100,150),
                    #                       interaction.depth=c(1,2,3),
                    #                       shrinkage=0.1,
                    #                       n.minobsinnode=10),
                    trControl=ctrl, 
                    metric="Accuracy")

modelo_gbm

modelo_gbm$bestTune


PROBA.GBM <- predict(modelo_gbm,newdata = data.test, type="prob")
PROBA.GBM <- PROBA.GBM[,2]
CLASE.GBM <- predict(modelo_gbm,newdata = data.test )

# Calcular el accuracy
accuracy_gbm <- mean(data.test$P701==CLASE.GBM);
accuracy_gbm

cm_gbm <- caret::confusionMatrix(CLASE.GBM,
                                 data.test$P701,
                                 positive="Si")

cm_gbm

# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.GBM,data.test$P701,plotROC = TRUE) -> auc_gbm
abline(0, 1,col="red")

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.GBM,real) -> logloss_gbm
```




### Modelo XGBoosting
```{r}
set.seed(2021)
modelo_xg <- train(P701 ~ ., 
                   data = data.train, 
                   method = "xgbTree", 
                   tuneLength=2,
                   trControl=ctrl, 
                   metric="Accuracy")

modelo_xg

modelo_xg$bestTune

modelo_xg$resample

plot(modelo_xg)


library(xgboost)
library(DiagrammeR)
xgb.plot.tree(model = modelo_xg$finalModel, trees = 1)
xgb.plot.tree(model = modelo_xg$finalModel, trees = 5)

PROBA.XG <- predict(modelo_xg,newdata = data.test, type="prob")
PROBA.XG <- PROBA.XG[,2]
CLASE.XG <- predict(modelo_xg,newdata = data.test)

# Calcular el accuracy
accuracy_xg <- mean(data.test$P701==CLASE.XG); 
accuracy_xg


cm_xg <- caret::confusionMatrix(CLASE.XG,
                                data.test$P701,
                                positive="Si")

cm_xg


# Curva ROC usando el paquete caTools
library(caTools)
colAUC(PROBA.XG,data.test$P701,plotROC = TRUE) -> auc_xg
abline(0, 1,col="red")
auc_xg

# Log-Loss
library(MLmetrics)
real <- as.numeric(data.test$P701)
real <- ifelse(real==2,1,0)
LogLoss(PROBA.XG,real) -> logloss_xg
logloss_xg
```

### Comparando la estabilidad del modelo

```{r}
modelos  <- list(KNN            = modelo_knn,
                 RL             = modelo_rl,
                 NAIVE          = modelo_nb,
                 SVC            = modelo_svc,
                 SVM            = modelo_svm,
                 RNA            = modelo_rna,
                 C5             = modelo_C5,
                 CART           = modelo_cart,
                 BAG            = modelo_bag,
                 RF             = modelo_rf,
                 ADA            = modelo_ada,
                 GBM            = modelo_gbm,
                 XGBoosting     = modelo_xg)

comparacion_modelos <- resamples(modelos) 
summary(comparacion_modelos)

modelCor(comparacion_modelos)

densityplot(comparacion_modelos,
            metric = "Accuracy",
            auto.key=TRUE)
```

### Comparacion de modelos

```{r}
algoritmos       <- c("KNN",
                      "Regresion Logistica",
                      "Naive Bayes",
                      "SVM Lineal",
                      "SVM",
                      "Red neuronal",
                      "Arbol C5",
                      "CART",
                      "Bagging",
                      "Random Forest",
                      "AdaBoosting",
                      "GBM",
                      "XGBoosting")

sensibilidad  <- c(cm_knn$byClass["Sensitivity"],
                    cm_rl$byClass["Sensitivity"],
                    cm_nb$byClass["Sensitivity"],
                    cm_svc$byClass["Sensitivity"],
                    cm_svm$byClass["Sensitivity"],
                    cm_rna$byClass["Sensitivity"],
                    cm_c5$byClass["Sensitivity"],
                    cm_cart$byClass["Sensitivity"],
                    cm_bag$byClass["Sensitivity"],
                    cm_rf$byClass["Sensitivity"],
                    cm_ada$byClass["Sensitivity"],
                    cm_gbm$byClass["Sensitivity"],
                    cm_xg$byClass["Sensitivity"])


especificidad <- c(cm_knn$byClass["Specificity"],
                    cm_rl$byClass["Specificity"],
                    cm_nb$byClass["Specificity"],
                    cm_svc$byClass["Specificity"],
                    cm_svm$byClass["Specificity"],
                    cm_rna$byClass["Specificity"],
                    cm_c5$byClass["Specificity"],
                    cm_cart$byClass["Specificity"],
                    cm_bag$byClass["Specificity"],
                    cm_rf$byClass["Specificity"],
                    cm_ada$byClass["Specificity"],
                    cm_gbm$byClass["Specificity"],
                    cm_xg$byClass["Specificity"])

accuracy      <- c(accuracy_knn,
                    accuracy_rl,
                    accuracy_nb,
                    accuracy_svc,
                    accuracy_svm,
                    accuracy_rna,
                    accuracy_c5,
                    accuracy_cart,
                    accuracy_bag,
                    accuracy_rf,
                    accuracy_ada,
                    accuracy_gbm,
                    accuracy_xg)

area_roc      <- c(auc_knn,
                    auc_rl,
                    auc_nb,
                    auc_svc,
                    auc_svm,
                    auc_rna,
                    auc_c5,
                    auc_cart,
                    auc_bag,
                    auc_rf,
                    auc_ada,
                    auc_gbm,
                    auc_xg)


logloss        <- c(logloss_knn,
                    logloss_rl,
                    logloss_nb,
                    logloss_svc,
                    logloss_svm,
                    logloss_rna,
                    logloss_c5,
                    logloss_cart,
                    logloss_bag,
                    logloss_rf,
                    logloss_ada,
                    logloss_gbm,
                    logloss_xg)

comparacion <- data.frame(algoritmos,
                          sensibilidad, 
                          especificidad,
                          accuracy,
                          area_roc,
                          logloss)

comparacion
```

Se han elegido 4 modelos que estan menos correlacionados -> knn, naive, bag, ada


### Ensamble de modelos
```{r}
set.seed(2021)
modelos <- as.caretList(modelos)

ensamble <- caretEnsemble(modelos)
summary(ensamble)

proba_ensam  <- predict(object = ensamble,
                        data.test[,predictores],
                        type='prob')
head(proba_ensam)

clase_ensam <- predict(object = ensamble,
                       data.test[,predictores])

cm_ensamble <- caret::confusionMatrix(clase_ensam,
                                      data.test[,target],
                                      positive="Si")

cm_ensamble$byClass["Sensitivity"] 
cm_ensamble$byClass["Specificity"] 
cm_ensamble$overall["Accuracy"]
```

### Stacking de modelos
```{r}
stackControl <- trainControl(method = "cv",
                             number = 10,
                             savePredictions = 'final', # To save out of fold predictions for best parameter combinantions
                             classProbs = T             # To save the class probabilities of the out of fold predictions
                             )

set.seed(2021)
stack.svm <-  caretStack(modelos,
                         method="svmLinear",      
                         tuneLength=5,
                         metric="Accuracy",  
                         trControl=stackControl)
 
#plot(stack.svm)

summary(stack.svm)

proba_stack  <- predict(object = stack.svm,
                        data.test[,predictores],
                        type='prob')

clase_stack  <- predict(object = stack.svm,
                        data.test[,predictores])

cm_stacking       <- caret::confusionMatrix(clase_stack,
                                            data.test[,target],
                                            positive="Si")


cm_stacking$byClass["Sensitivity"] 
cm_stacking$byClass["Specificity"] 
cm_stacking$overall["Accuracy"]
```

Umbral optimo con lo obtenido en el stacking

```{r}
library(pROC)
roc <- roc(data.test[,target],proba_stack)
plot.roc(roc, 
         legacy.axes=TRUE,
         print.thres=TRUE,
         auc.polygon=TRUE,
         col="blue",
         print.auc=TRUE,
         auc.polygon.col="lightblue",
         xlab="1-Especificidad", 
         ylab="Sensibilidad")

umbral <- pROC::coords(roc, "best")$threshold
umbral

clase_stack2 <- as.factor(ifelse(proba_stack>umbral,
                                    "Si", "No"))

cm_stack2 <- caret::confusionMatrix(clase_stack2, 
                                    data.test[,target],
                                    positive="Si")

cm_stack2$byClass["Sensitivity"] 
cm_stack2$byClass["Specificity"] 
cm_stack2$overall["Accuracy"]
```
Se concluye que con el modelo obtenido luego del stacking se tiene un accuracy de 0.6994907 superando a los accuracy de los modelos
por usados por separado y tambien superando a lo obtenido con el umbral optimo.
